[training]

lang                       = de
batch_size                 = 32

# Optimizer for training: (adadelta, adam, rmsprop)
optimizer                  = adam

# Learning rate
learning_rate              = 0.0002

# Clip gradients to this norm
max_gradient_norm          = 1.0

[model]

# RNN cell for encoder and decoder, default: lstm
cell_type                  = lstm  

# Attention mechanism: (bahdanau, luong), default: bahdanau
attention_type             = bahdanau

# Number of hidden units in each layer
hidden_units               = 512

# Number of layers in each encoder and decoder
depth                      = 3

# Embedding dimensions of encoder and decoder inputs
embedding_size             = 500

# Source vocabulary size
# num_encoder_symbols        = 30000

# Target vocabulary size
# num_decoder_symbols        = 30000

# Use residual connection between layers
use_residual               = True

# Use input feeding method in attentional decoder
attn_input_feeding         = False

# Use dropout in each rnn cell
use_dropout                = True

# Dropout probability for input/output/state units (0.0: no dropout)
dropout_rate               = 0.3

# Use half precision float16 instead of float32 as dtype
use_fp16                   = False

[decode]
beam_width                 = 12
max_decode_step            = 500

