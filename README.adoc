ifndef::imagesdir[:imagesdir: doc]
Zamia AI
========

The Zamia AI project provides all the components needed to build a free, open
source end-to-end natural language processing A.I. system. All components and
models used are free (as in speech), all the sources are open and freely
available not just for the software components but also all training data as
well as the tools needed to create the neural network models for the system.

This means you can build entirely self-contained A.I. systems with no dependencies
on proprietary components, data sources or cloud services that you do not control. 
Of course, you can always add or replace individual components by proprietary or cloud 
based ones if you wish but you can also take it to the other extreme and build 
completely self-contained systems that are capable of running in offline mode
with no network connectivity required at all. We like to call this concept
cloudless computing.

Typical systems that can be built based on Zamia AI include text based ones (i.e.
some sort of chat bot) and speech based ones (i.e. speech based assistants). 
Possible applications range from rather simple fun or educational toys up to
serious systems for command and control in industrial or automotive contexts
or assistive technology and healthcare solutions.

Requirements
------------

*Note*: probably incomplete.

* Python 2.7 with nltk, numpy, ...
* tensorflow
* XSB Prolog
* SQLite
* from my other repositories: py-nltools, py-xsb-prolog

Setup Notes
-----------

Just some rough notes on the environment needed to get these scripts to run. This is in no way a complete set of
instructions, just some hints to get you started.

create a file called `~/.airc`:

```ini
[db]
url                 = sqlite:///ai.db

[semantics]
toplevel            = chatbot
server_host         = dagobert
server_port         = 8302

xsb_root            = /opt/xsb-3.8.0

[weather]
api_key             = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
city_id             = 2825297
city_pred           = stuttgart

[context]
place               = 'http://www.wikidata.org/entity/Q1022'
channel             = 'http://www.wikidata.org/entity/Q795291'
time                = today
```

compile the knowledge base:
```bash
./ai_cli.py compile all
```
train a model:
```bash
./ai_cli.py train models/en_3_512.ini
```
alternatively, you can download a pre-built model here: http://goofy.zamia.org/zamia-ai/

chat with the model:
```bash
[guenter@dagobert zamia-ai]$ ./ai_cli.py chat -m models/en_3_512.ini chatbot
[xsb_configuration loaded]

...

INFO:root:model restored from models/en_3_512/latest.ckpt
ai> hello computer
INFO:root:RESP: [   0.0] Hi! 
ai> how are you?
INFO:root:RESP: [   0.0] Great, thanks. How do you feel today? 
ai> I am well
INFO:root:RESP: [   0.0] Excellent! 
ai> what is 2 times 2
INFO:root:RESP: [ 104.0] 4 
ai> correct
INFO:root:RESP: [   0.0] Good. 
ai> where do you live?
INFO:root:RESP: [ 100.0] I am located in Stuttgart. 
ai> what is your favorite movie?
INFO:root:RESP: [ 100.0] 2001: A Space Odyssey by Stanley Kubrick 
ai> How about the weather?
INFO:root:RESP: [   2.0] today there will be some clouds in Stuttgart with temperatures between 1 and 7 degrees. 
ai> Tell me something about yourself
INFO:root:RESP: [   0.0] Please tell me how I can help you. 
ai> Where was Stephen King born?
INFO:root:RESP: [   4.0] Stephen King was born in Portland, I think. 
```

Architecture
------------

Right now the rough idea is to use XSB Prolog at the core for knowledge storage, processing and
reasoning plus a seq2seq model to map natural language to python code which queries the Prolog engine.

ifndef::env-github[]
[ditaa,"flow"]
....
  +-------------------+        +-------------------+
  |  Natural language |        |  Structured Data  |  
  |                {d}|        |  (RDF/LDF, AIML)  |
  +-------------------+        |                {d}|
            |                  +-------------------+
            |                          |    |
            v                          |    |
 +---------------------+               |    |
 | Deep Neural Network |<-- training --+    |
 |       seq2seq       |                    |
 +---------------------+                    |
            |                               |
            |                               v
            v                 +------------------+
    /---------------\         |    XSB Prolog    |
    |  Python code  | <------>|     Reasoner     |     
    \---------------/         |  Knowledge Base  |
            |                 +------------------+
            |                 
            v
  +-------------------+
  |     responses     |
  |                {d}|
  +-------------------+

....
endif::env-github[]
ifdef::env-github[]
image::flow.png[Dataflow]
endif::env-github[]

From a code organization perspective Zamia AI's architecture looks like this:

ifndef::env-github[]
[ditaa,"highlevel"]
....
+------------------------------------------------------------------------------------------------+
|                                         Applications                                           |
|                                                                                                |
+------------------------------------------------------------------------------------------------+
         ^                                      ^                                       |
         |                                      |                                       v
         |                                response/actions                       input/utterance
         |                                      ^                                       |
         |                                      |                                       v
         |            +--------------------------------------------------------------------------+
         |            |                         |        Zamia AI                       |        |
         |            |  +---------------+      |                                       |        |
         |            |  |  Data-Tools   |      |                                       v        |
         |            |  |               |      |          /-------------\         +---------+   |
         |            |  | - LDF mirror  |      +----------| Python code |<--------| seq2seq |   |
         |            |  | - RDF2Prolog  |                 \-------------/         |  model  |   |
         |            |  | - AIML import |                        ^                +---------+   |
         |            |  | - chat import |                        |                     ^        |
         |            |  +---------------+                        v                     |        |
         |            |         |                +-----------------------+                       |
         |            |         v                |      XSB Prolog       |            train      |
         |            |  +---------------+       +-----------------------+                       |
         |            |  |   Modules     |              ^                               |        |
         |            |  |               |              |                               |        |
         |            |  | - data        |    +------------------+                 /----------\  |
         |            |  | - base        |    |                  |                 |          |  |
         |            |  | - dialog      |--->|     compiler     |--- Python     ->| database |  |
         |            |  | - personality |    |    macro engine  |--- utterances ->|          |  |
         |            |  | - weather     |    |                  |                 \----------/  |
         |            |  |   ...         |    +------------------+                               |
         |            |  +---------------+                                                       |
         |            |                                                                          |
         |            +--------------------------------------------------------------------------+
         |                                               ^
         |                                               |
         v                                               v
 +-----------------------------------------------------------------------------------------------+
 |                                           nltools                                             |
 | +-----------+  +-----------+  +------------+                                                  |
 | | tokenizer |  | phonetics |  | threadpool |                                                  |
 | +-----------+  +-----------+  +------------+                                                  |
 |                                                                                               |
 |      +-----------+               +-----------+     +-----------+ +-----------+ +-----------+  |
 |      |    tts    |               |    asr    |     |    vad    | |    g2p    | |   audio   |  |
 |      +-----------+               +-----------+     +-----------+ +-----------+ +-----------+  |
 |            |                           |                 |             |             |        |
 +-----------------------------------------------------------------------------------------------+
              |                           |                 |             |             |          
     +--------+---------+          +------+----+            |             |             |
     |        |         |          |           |            |             |             |
     v        v         v          v           v            v             v             v
 +------+ +--------+ +------+  +-------+ +-----------+ +--------+    +----------+ +------------+ 
 | mary | | eSpeak | | pico |  | kaldi | | cmusphinx | | webrtc |    | sequitur | | pulseaudio |
 +------+ +--------+ +------+  +-------+ +-----------+ +--------+    +----------+ +------------+
....
endif::env-github[]
ifdef::env-github[]
image::highlevel.png[Highlevel Diagram]
endif::env-github[]

One of the key features of the current setup is the way training data is stored/generated.
I am using a modularized approach here (see the modules/ directory for humble beginnings of this)
where I store snippets of natural language which uses a macro system for somewhat rule-based
generation of language examples (optionally incorporating data from the prolog knowledge base) 
and python code to execute it.

Knowledge Base Notes
--------------------

NOTE: at the time of this writing the general architecture of this system is still in flux, therefore documentation will
probably be more or less outdated.

// For documentation on ZamiaAI semantic processing, see <<doc/semantics#,semantics>>.

=== Context Provided by the Framework

Implicit argument `c` of type `AIContext` gives access to the dialog environment, kernal and response generation:

```python
c.kernal      # Kernal
c.inp         # current input string
c.user        # current user
c.realm       # current realm
c.lang        # current language (e.g. 'en', 'de')
c.test_mode   # True when running tests, False otherwise
c.current_dt  # datetime.datetime.now()
```

=== Memory

Besides making prolog `assertz` calls, a simple memory mechanism is provided for persistent data storage:

```python
c.kernal.mem_set(c.user, 'f1ent', film)
film = c.kernal.mem_get(c.user, 'f1ent')

c.kernal.mem_push(c.user, 'f1ent', film)
film = c.kernal.mem_get_multi(c.user, 'f1ent')
```

=== Response

To generate responses, call

```python
c.resp(resp, score, action, action_arg) # Response generation
```

=== data-tools

==== RDF

RDF data can be mirrored and converted to Prolog using the scripts found in `data-tools/rdf`.

Example: mirror the wikidata subset and generate Prolog from it:

```bash
cd data-tools/rdf
edit config.py as needed

./ldfmirror.py -o rdf/wd_sub.n3
./rdf2prolog.py -o ../../modules/data/wd_sub.pl rdf/wd_sub.n3
cd ../..
./ai_cli.py compile data
```

==== AIML data

AIML sources can be converted into CSV which can then be aligned with existing modules to 
produce additional training scripts:

```bash
pushd data-tools/aiml
./chatterbots2csv.sh
popd
data-tools/csv/csv2py.py -o modules/bots/bots.py data-tools/aiml/all_bots.csv 
```

Links
-----

* Code: https://github.com/gooofy/zamia-ai

Language Model
--------------

dump sentences from training data for LM generation:

```bash
./ai_cli.py utterances 
```

or to dump out a set of 20 random utterances which contain words not covered by the dictionary:

```bash
./ai_cli.py utterances -d ../speech/data/src/speech/de/dict.ipa -n 20
```

License
-------

My own scripts as well as the data I create is LGPLv3 licensed unless otherwise noted in the script's copyright headers.

Some scripts and files are based on works of others, in those cases it is my
intention to keep the original license intact. Please make sure to check the
copyright headers inside for more information.

Author
------

Guenter Bartsch <guenter@zamia.org>

