Zamia AI
========

Various scripts that one day could form a complete A.I. system. 

NOTE: If you're looking for the scripts that I am using to build my VoxForge models, those
are over at https://github.com/gooofy/speech.

Right now the rough idea is to use a custom prolog system (AI-Prolog) at the core for knowledge storage, processing and
reasoning plus a seq2seq model to map natural language to prolog.

```
natural language -> [ tokenizer ] -> tokens -> [ seq2seq model ] -> prolog -> [ AI-Prolog engine ] -> say/action preds
```

One of the key features of the current setup is the way training data is stored/generated.
I am using a modularized approach here (see the modules/ directory for humble beginnings of this)
where I store snippets of natural language which uses a macro system for somewhat rule-based
generation of language examples (optionally incorporating data from the prolog knowledge base) 
and prolog code to execute it.

Knowledge Base Notes
--------------------

NOTE: at the time of this writing the general architecture of this system is still in flux, therefore documentation will
probably be more or less outdated.

// For documentation on ZamiaAI semantic processing, see <<doc/semantics#,semantics>>.

=== Context Provided by Framework

Implicit argument `C` which is initialized to a unique prolog constant representing the
current round (i.e. interaction step). 

e.g. `C = context42`

assertions made by framework:

```prolog
user    (context42, userDefault).
lang    (context42, en).
tokens  (context42, ['hi', 'there']).
time    (context42, "2004-06-14T23:34:30").
prev    (context42, context41).
```

any assertions of the form

```prolog
context (context41, key, value).
mem     (context41, key, value).
```

made on the previous context will be made again (i.e.: persist) on the current context,
context assertions will also serve as additional inputs to subsequent neural net runs

=== Pseudo-Variables/-Predicates

This is an extension to standard prolog syntax found in AI-Prolog to make "variable" setting and access
easier:

```
C:user        -> user (C, X)

C:user:name   -> user (C, X), name (X, Y)

self:name     -> name (self, X)

self:label|de -> label (self, de, X)

```

=== Response

The response generated by prolog processing is comprised of assertions such as:

```prolog
say    (C, token).
action (C, [args]).
score  (C, score).
```

=== data-tools

==== RDF

RDF data can be mirrored and converted to AI-Prolog using the scripts found in `data-tools/rdf`.

Example: mirror the wikidata subset and generate AI-Prolog from it:

```bash
cd data-tools/rdf
edit config.py as needed

./ldfmirror.py -o rdf/wd_sub.n3
./rdf2prolog.py -o ../../modules/data/wd_sub.pl rdf/wd_sub.n3
cd ../..
./ai_cli.py compile data
```

==== AIML / Chat data

Data from AIML sources can be converted to `chat` format which can then be turned into AI-Prolog training scripts:

```bash
pushd data-tools/aiml
./chatterbots2chat.sh
popd
data-tools/chat/chat2aip.py -o modules/chat/chat.aip data-tools/aiml/bots_en/alice_new.chat data-tools/aiml/bots_en/square_bear.chat data-tools/aiml/bots_en/dobby.chat data-tools/aiml/bots_en/emmie.chat data-tools/aiml/bots_en/proalias.chat data-tools/aiml/bots_en/rosie.chat data-tools/aiml/bots_en/runabot.chat tmp/chat_corpus/movie_subtitles_en.txt 
```

module initialization / test setup
----------------------------------

when a aiprolog module is loaded for processing, a special target

```prolog
    init("module_name")
```

is searched for and actions produced from this search are executed. this mechanism can
be used to initialize the module through prolog code in general and setup default 
context values in particular, as 

```
    ai:curin  ai:user aiu:default
```

is set during execution.

Additionally, before running each test, a special target
```prolog
    test_setup("module_name")
```
is searched. 
```
    ai_curin  ai:user aiu:test
```
is set during execution

Links
-----

* Code: https://github.com/gooofy/zamia-ai

Requirements
------------

*Note*: very incomplete.

* Python 2.7 with nltk, numpy, ...
* tensorflow
* from my other repositories: py-nltools, sparqlalchemy

Setup Notes
-----------

Just some rough notes on the environment needed to get these scripts to run. This is in no way a complete set of
instructions, just some hints to get you started.

`~/.airc`:

```ini
[db]
url                 = postgresql://semantics:password@dagobert:5432/zamia_ai

[semantics]
modules             = data, config, base, dt, humans, weather, media, movies, literature, tech, smalltalk, politics, personality, chat
server_host         = dagobert
server_port         = 8302

[weather]
api_key             = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
city_id             = 2825297
city_pred           = stuttgart

[context]
place               = 'http://www.wikidata.org/entity/Q1022'
cannel              = 'http://www.wikidata.org/entity/Q795291'
time                = today
```

Language Model
--------------

dump sentences from training data for LM generation:

```bash
./ai_cli.py utterances 
```

or to dump out a set of 20 random utterances which contain words not covered by the dictionary:

```bash
./ai_cli.py utterances -d ../speech/data/src/speech/de/dict.ipa -n 20
```

License
-------

My own scripts as well as the data I create is LGPLv3 licensed unless otherwise noted in the script's copyright headers.

Some scripts and files are based on works of others, in those cases it is my
intention to keep the original license intact. Please make sure to check the
copyright headers inside for more information.

Author
------

Guenter Bartsch <guenter@zamia.org>

